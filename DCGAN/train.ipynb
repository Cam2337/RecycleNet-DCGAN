{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DCGAN model...\n",
      "Start Trainng...\n",
      "Start Epoch...0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nparab\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][0/1583]\tLoss_D: 1.2849\tLoss_G: 54.2319\tD(x): 0.6586\tD(G(z)): 0.4923 / 0.0000\n"
     ]
    }
   ],
   "source": [
    "################################# DCGAN-train ###############################################\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import model_DCGAN as model\n",
    "\n",
    "############################################ DATA LOADING #################################\n",
    "# data-root\n",
    "dataroot = \"C:/Users/nparab/Desktop/CS230/project/finalproject/cs230-project/dataset/celeba\"\n",
    "#dataroot = '/myData/img_align_celeba/img_align_celeba'\n",
    "\n",
    "#number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# training batch-size\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "#Load dataset and resize\n",
    "dataset = dset.ImageFolder( root=dataroot,\n",
    "                            transform=transforms.Compose([\n",
    "                              transforms.Resize(model.image_size),\n",
    "                              transforms.CenterCrop(model.image_size),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ]))\n",
    "#Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "\n",
    "############################################ DATA LOADING (end) #################################\n",
    "\n",
    "\n",
    "########################################### TRAINING ###########################################\n",
    "\n",
    "# batch of input latent vectors\n",
    "fixed_noise = torch.randn(model.ngv, model.gvSize, 1, 1, device=model.device)\n",
    "\n",
    "\n",
    "####### 1.Hyperparameter tuning ######\n",
    "\n",
    "# learning-rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# adam optimizer\n",
    "beta1 = 0.5\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "####### Hyperparameter tuning (end) ######\n",
    "\n",
    "\n",
    "####### 2.LOSS Function and Optimizer ######\n",
    "\n",
    "# BCELoss function\n",
    "lossF = nn.BCELoss()\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optD = optim.Adam(model.netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "optG = optim.Adam(model.netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
    "\n",
    "####### LOSS Function and Optimizer(end) ######\n",
    "\n",
    "\n",
    "####### 3. Training Loop ######\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "# labels\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "print(\"Start Trainng...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Start Epoch...\" + str(epoch))\n",
    "    for i, data in enumerate(dataloader):\n",
    "        \n",
    "        real = data[0].to(model.device)\n",
    "        b_size = real.size(0)\n",
    "        labels_real = torch.full((b_size,), real_label, device=model.device)\n",
    "        labels_fake = torch.full((b_size,), fake_label, device=model.device)\n",
    "    \n",
    "        ## **Train real data through Discriminator** ##\n",
    "        model.netD.zero_grad()\n",
    "        #forward prop\n",
    "        outputD_real = model.netD(real)\n",
    "        #loss\n",
    "        lossD_real = lossF(outputD_real, labels_real)\n",
    "        # backward prop\n",
    "        lossD_real.backward()\n",
    "        #stats (D)\n",
    "        D_x = outputD_real.mean().item()\n",
    "\n",
    "        \n",
    "        ## **Train fake data through Discriminator** ##\n",
    "        #forward prop\n",
    "        noise = torch.randn(b_size, model.gvSize, 1, 1, device=model.device)\n",
    "        fake = model.netG(noise)\n",
    "        outputD_fake = model.netD(fake.detach())\n",
    "        #loss\n",
    "        lossD_fake = lossF(outputD_fake,labels_fake)\n",
    "        # backward prop\n",
    "        lossD_fake.backward()\n",
    "        #stats\n",
    "        D_G_z1 = outputD_fake.mean().item()\n",
    "        \n",
    "        #total discriminator loss (real + fake)\n",
    "        lossD = lossD_real + lossD_fake\n",
    "        #update\n",
    "        optD.step()\n",
    "\n",
    "        \n",
    "        ## **Update Generator** ##\n",
    "        model.netG.zero_grad()\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = model.netD(fake)\n",
    "        #loss\n",
    "        lossG = lossF(output, labels_real)\n",
    "        # backward pass (D)\n",
    "        lossG.backward()\n",
    "        #update\n",
    "        optG.step()\n",
    "       \n",
    "        #stats (D)\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        \n",
    "        #### LOG Training stats ####\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                    % (epoch, num_epochs, i, len(dataloader),\n",
    "                     lossD.item(), lossG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # # Save Losses for plotting later\n",
    "        G_losses.append(lossG.item())\n",
    "        D_losses.append(lossD.item())\n",
    "\n",
    "        # # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = model.netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "\n",
    "#RESULTS\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(model.device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
